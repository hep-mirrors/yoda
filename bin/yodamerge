#! /usr/bin/env python

"""\
%prog [-o outfile] <yodafile1> <yodafile2> ...

Merge analysis objects from multiple YODA files, combining the statistics of
objects whose names are found in multiple files. May be used either to merge
disjoint collections of data objects, or to combine multiple statistically
independent runs of the same data objects into one high-statistics run.

By default the output is written to stdout since we can't guess what would be
a good automatic filename choice! Us the -o option to provide an output filename.

If all the input histograms with a particular path are found to have the same
normalization, and they have ScaledBy attributes indicating that a histogram
weight scaling has been applied in producing the input histograms, each
histogram in that group will be first unscaled by their appropriate factor, then
merged, and then re-normalized to the target value. Otherwise the weights from
each histogram copy will be directly added together with no attempt to guess an
appropriate normalization.

IMPORTANT: note from the above that this script can't work out what to do
re. scaling and normalization of output histograms from the input data files
alone. It may be possible (although unlikely) that input histograms have the
same normalization but are meant to be added directly. It may also be the case
(and much more likely) that histograms which should be normalized to a common
value will not trigger the appropriate treatment due to e.g. statistical
fluctuations in each run's calculation of a cross-section used in the
normalization. And anything more complex than a global scaling (e.g. calculation
of a ratio or asymmetry) cannot be handled at all with a post-hoc scaling
treatment. Please use this script as a template if you need to do something more
specific.

NOTE: there are many possible desired behaviours when merging runs, depending on
the factors above as well as whether the files being merged are of homogeneous
type, heterogeneous type, or a combination of both. It is tempting, therefore,
to add a large number of optional command-line parameters to this script, to
handle these cases. Experience from Rivet 1.x suggests that this is a bad idea:
if a problem is of programmatic complexity then a command-line interface which
attempts to solve it in general is doomed to both failure and unusability. Hence
we will NOT add extra arguments for extra per-file scaling factors, path pattern
matching behaviours, identifying 'types' of run, etc., etc.: if you need to
merge data files in such complex ways, please use this script as a simple template
around which to write logic that satisfies your particular requirements.

TODO:
 * Use the generic yoda.read() function?
"""

import yoda, optparse, operator, itertools

parser = optparse.OptionParser(usage=__doc__)
parser.add_option('-o', '--output', default='-', dest='OUTPUT_FILE')
parser.add_option('-N', '--normalize-all', action="store_true", default=False, dest='NORMALIZE_ALL')
opts, filenames = parser.parse_args()

## Put the incoming objects into a dict from each path to a list of histos
analysisobjects_in = {}
for filename in filenames:
    # TODO: also accept AIDA, ROOT, ... inputs?
    aos = yoda.readYODA(filename)
    for ao in aos:
        analysisobjects_in.setdefault(ao.path, []).append(ao)

analysisobjects_out = {}
for p, aos in analysisobjects_in.iteritems():
    # print p, len(aos)
    ## Check that types match, and just output the first one if they don't
    if not all(type(ao) == type(aos[0]) for ao in aos):
        print "Several types of analysis object found at path %s: cannot be merged"
        analysisobjects_out[p] = aos[0]
        continue
    ## Check whether normalizations match
    normto = None
    if hasattr(aos[0], "totalDbn"):
        ## In the absence of better info, we use the norm as a heuristic to change the merging behaviour.
        # print tuple(ao.totalDbn.sumW for ao in aos)
        if opts.NORMALIZE_ALL or \
               all(ao.totalDbn.sumW == 0 or abs(ao.totalDbn.sumW - aos[0].totalDbn.sumW)/aos[0].totalDbn.sumW < 1e-3 for ao in aos):
            ## Compute a target normalization from the 1/scalefactor-weighted norms of each run
            assert all(ao.annotations.has_key("ScaledBy") for ao in aos)
            wtot = sum(1/float(ao.annotations["ScaledBy"]) for ao in aos)
            normto = sum(ao.totalDbn.sumW / float(ao.annotations["ScaledBy"]) for ao in aos) / wtot
    for ao in aos:
        ## Unscale first if normto != None
        if normto:
            ao.scaleW(1/float(ao.annotations("ScaledBy")))
        if not analysisobjects_out.has_key(p):
            analysisobjects_out[p] = ao
        elif hasattr(ao, "__iadd__"):
            analysisobjects_out[p] += ao
        elif hasattr(ao, "__add__"):
            analysisobjects_out[p] = ao + analysisobjects_out[p]
        else:
            print "Analysis object %s of type %s cannot be merged" % (p, str(type(ao)))
            break
    ## Renormalize after adding if normto != None
    if normto:
        analysisobjects_out[p].normalize(normto)

yoda.write(analysisobjects_out.values(), opts.OUTPUT_FILE)

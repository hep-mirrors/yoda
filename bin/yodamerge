#! /usr/bin/env python

"""\
%prog [-o outfile] <yodafile1> <yodafile2> ...

Merge analysis objects from multiple YODA files, combining the statistics of
objects whose names are found in multiple files. May be used either to merge
disjoint collections of data objects, or to combine multiple statistically
independent runs of the same data objects into one high-statistics run.

By default the output is written to stdout since we can't guess what would be
a good automatic filename choice! Us the -o option to provide an output filename.

If all the input histograms with a particular path are found to have the same
normalization, and they have ScaledBy attributes indicating that a histogram
weight scaling has been applied in producing the input histograms, each
histogram in that group will be first unscaled by their appropriate factor, then
merged, and then re-normalized to the target value. Otherwise the weights from
each histogram copy will be directly added together with no attempt to guess an
appropriate normalization.

IMPORTANT: note from the above that this script can't work out what to do
re. scaling and normalization of output histograms from the input data files
alone. It may be possible (although unlikely) that input histograms have the
same normalization but are meant to be added directly. It may also be the case
(and much more likely) that histograms which should be normalized to a common
value will not trigger the appropriate treatment due to e.g. statistical
fluctuations in each run's calculation of a cross-section used in the
normalization. And anything more complex than a global scaling (e.g. calculation
of a ratio or asymmetry) cannot be handled at all with a post-hoc scaling
treatment. The -N/--normalize-all command line option will force all histograms
to be treated as if they are normalized in the input, which can be useful if
you know that all the output histograms are indeed of this nature.
Please use this script as a template if you need to do something more specific.

NOTE: there are many possible desired behaviours when merging runs, depending on
the factors above as well as whether the files being merged are of homogeneous
type, heterogeneous type, or a combination of both. It is tempting, therefore,
to add a large number of optional command-line parameters to this script, to
handle these cases. Experience from Rivet 1.x suggests that this is a bad idea:
if a problem is of programmatic complexity then a command-line interface which
attempts to solve it in general is doomed to both failure and unusability. Hence
we will NOT add extra arguments for extra per-file scaling factors, path pattern
matching behaviours, identifying 'types' of run, etc., etc.: if you need to
merge data files in such complex ways, please use this script as a simple template
around which to write logic that satisfies your particular requirements.

TODO:
 * Use the generic yoda.read() function so we can use multiple formats? ROOT input
   _could_ partially work, although won't include all statistical weights.
"""

import yoda, optparse, operator, itertools

parser = optparse.OptionParser(usage=__doc__)
parser.add_option('-o', '--output', default='-', dest='OUTPUT_FILE')
parser.add_option('-N', '--normalize-all', action="store_true", default=False, dest='NORMALIZE_ALL')
opts, filenames = parser.parse_args()

## Put the incoming objects into a dict from each path to a list of histos
analysisobjects_in = {}
for filename in filenames:
    # TODO: also accept AIDA, ROOT, ... inputs?
    aos = yoda.readYODA(filename)
    for aopath, ao in aos.iteritems():
        analysisobjects_in.setdefault(aopath, []).append(ao)

analysisobjects_out = {}
for p, aos in analysisobjects_in.iteritems():
    # print p, len(aos)
    ## Check that types match, and just output the first one if they don't
    if not all(type(ao) == type(aos[0]) for ao in aos):
        print "WARNING: Several types of analysis object found at path %s: cannot be merged"
        analysisobjects_out[p] = aos[0]
        continue
    ## Check whether normalizations match
    normto = None
    if hasattr(aos[0], "totalDbn"):
        ## In the absence of better info, we use the norm as a heuristic to change the merging behaviour.
        #print tuple(ao.integral() for ao in aos)
        if opts.NORMALIZE_ALL or \
               all(ao.totalDbn.sumW == 0 or abs(ao.totalDbn.sumW - aos[0].totalDbn.sumW)/aos[0].totalDbn.sumW < 1e-3 for ao in aos):
            ## Try to compute a target normalization from the 1/scalefactor-weighted norms of each run
            if all(ao.annotations.has_key("ScaledBy") for ao in aos):
                wtot = sum(1/float(ao.annotations["ScaledBy"]) for ao in aos)
                normto = sum(ao.totalDbn.sumW / float(ao.annotations["ScaledBy"]) for ao in aos) / wtot
            else:
                print "WARNING: Abandoning normalized merge of path %s because not all inputs have ScaledBy attributes" % p
    ## Loop over and combine all data objects for this path
    for ao in aos:
        ## Unscale first if normto != None
        if normto:
            ao.scaleW(1/float(ao.annotations["ScaledBy"]))
        ## Combine as far as supported by this data type
        if not analysisobjects_out.has_key(p):
            analysisobjects_out[p] = ao
        elif hasattr(ao, "__iadd__"):
            analysisobjects_out[p] += ao
        # elif hasattr(ao, "__add__"):
        #     analysisobjects_out[p] = ao + analysisobjects_out[p]
        else:
            print "WARNING: Analysis object %s of type %s cannot be merged" % (p, str(type(ao)))
            break
    ## Re-normalize after adding if normto != None
    if normto:
        analysisobjects_out[p].normalize(normto)

## Write output
yoda.writeYODA(analysisobjects_out, opts.OUTPUT_FILE)
